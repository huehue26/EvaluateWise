{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever,EnsembleRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda,RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from azure.search.documents.indexes.models import (SearchableField,SearchField,SearchFieldDataType,SimpleField)\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load enviornment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI settings\n",
    "openai_api_key: str = os.environ.get(\"OPENAI_KEY\")\n",
    "openai_api_version: str = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Search Settings\n",
    "vector_store_address: str = os.environ.get(\"SEARCH_SERVICE\")\n",
    "vector_store_password: str = os.environ.get(\"AZURE_SEARCH_KEY\")\n",
    "index_name:str = \"langchain-vector-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Langchain Test\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPDF loader for parsing PDF files\n",
    "loader = PyPDFLoader(r'Books\\yolov7.pdf')\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-\\ning [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI\\nprocessing unit (MediaTek), and the AI SoCs (Kneron), are\\nall NPUs. Some of the above mentioned edge devices focus\\non speeding up different operations such as vanilla convolu-\\ntion, depth-wise convolution, or MLP operations. In this pa-\\nper, the real-time object detector we proposed mainly hopes\\nthat it can support both mobile GPU and GPU devices from\\nthe edge to the cloud.\\nIn recent years, the real-time object detector is still de-\\nveloped for different edge device. For example, the devel-\\nFigure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on\\nproducing low-power single-chip and improving the infer-\\nence speed on edge CPU. As for methods such as YOLOX\\n[21] and YOLOR [81], they focus on improving the infer-\\nence speed of various GPUs. More recently, the develop-\\nment of real-time object detector has focused on the de-\\nsign of efﬁcient architecture. As for real-time object de-\\ntectors that can be used on CPU [54, 88, 84, 83], their de-\\nsign is mostly based on MobileNet [28, 66, 27], ShufﬂeNet\\n[92, 55], or GhostNet [25]. Another mainstream real-time\\nobject detectors are developed for GPU [81, 21, 97], they\\nmostly use ResNet [26], DarkNet [63], or DLA [87], and\\nthen use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in\\nthis paper are different from that of the current mainstream\\nreal-time object detectors. In addition to architecture op-\\ntimization, our proposed methods will focus on the opti-\\nmization of the training process. Our focus will be on some\\noptimized modules and optimization methods which may\\nstrengthen the training cost for improving the accuracy of\\nobject detection, but without increasing the inference cost.\\nWe call the proposed modules and optimization methods\\ntrainable bag-of-freebies.\\n1arXiv:2207.02696v1  [cs.CV]  6 Jul 2022' metadata={'source': 'Books\\\\yolov7.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Details\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\":\"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\":True}\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(model_name=model_name,model_kwargs=model_kwargs,encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_test = hf.embed_query(\"Hi this is krish\")\n",
    "len(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Semantic Chunker\n",
    "text_splitter = SemanticChunker(hf)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = list(map(lambda x: x.page_content,pages))\n",
    "metadata = list(map(lambda x: x.metadata,pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\n",
      "detectors\n",
      "Chien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\n",
      "1Institute of Information Science, Academia Sinica, Taiwan\n",
      "kinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\n",
      "Abstract\n",
      "YOLOv7 surpasses all known object detectors in both\n",
      "speed and accuracy in the range from 5 FPS to 160 FPS\n",
      "and has the highest accuracy 56.8% AP among all known\n",
      "real-time object detectors with 30 FPS or higher on GPU\n",
      "V100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\n",
      "AP) outperforms both transformer-based detector SWIN-\n",
      "L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\n",
      "509% in speed and 2% in accuracy, and convolutional-\n",
      "based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\n",
      "FPS A100, 55.2% AP) by 551% in speed and 0.7% AP\n",
      "in accuracy, as well as YOLOv7 outperforms: YOLOR,\n",
      "YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\n",
      "DETR, DINO-5scale-R50, ViT-Adapter-B and many other\n",
      "object detectors in speed and accuracy. Moreover, we train\n",
      "YOLOv7 only on MS COCO dataset from scratch without\n",
      "using any other datasets or pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = text_splitter.create_documents(page_content)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = hf.embed_query\n",
    "\n",
    "# Define fields for the Azure AI Search Index\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_function(\"Text\")),\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field to store the title\n",
    "    SearchableField(\n",
    "        name=\"title\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field for filtering on document source\n",
    "    SimpleField(\n",
    "        name=\"source\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate the vector store client\n",
    "vector_store:AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,azure_search_key=vector_store_password,index_name=index_name,embedding_function=hf.embed_query,fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YzY4NmZhMWYtNDEyMy00Njg4LTkwNTUtNzI3YTg2NTE1OGZk',\n",
       " 'OTkzNzk0Y2EtYmIyMC00ZDE5LWFjNjItYzRjNDQyM2QxM2M1',\n",
       " 'MmRmZTVmNDUtZDVjOS00ZGI5LTk5ZGEtNTgyYWQ1ZTEwOGQz',\n",
       " 'ZTFjMTQzOWQtZmUzYS00ZmZhLWI2OGItYjcyYTFmNWI3OTMz',\n",
       " 'ZDVmN2JhYWQtNDg2OC00ZmM5LTk4M2UtNjZhZjA0NmExYjZm',\n",
       " 'ZDlhNGUyOTktNTM3NS00YjViLTg5OGQtNWExY2NlMDA0ZTEw',\n",
       " 'NzU0ODY0ZjctOTcwNS00YTQ0LWJmYzAtNjg0MGQ0ZTk4NzY3',\n",
       " 'NWFiZGRhNTQtMTM3Yi00NWVkLThjNjItZGY1MTBhYmNiMjA5',\n",
       " 'NTZjZmE5ZTAtNWU1NS00ZmY4LTgzMGMtYTQ4YmFjOGM3ZTQz',\n",
       " 'N2ZlOTIzNzYtZjAxMy00ZmY5LWJlZDEtMGJjOThiZDAxMGFk',\n",
       " 'OGIyN2IwMDgtOTlhYi00YzA2LTllOTktODFmMzUxZjUwYmIw',\n",
       " 'NDZiMjIwNWItNTVhYy00MzMwLTkzYWItMjA2NjMzMWVkMGU1',\n",
       " 'OGYyMGY5ZDctNGQ4ZC00MzQ3LWE0MWEtOTkxMDU0ZmQwYTc0',\n",
       " 'Y2Y3YjQ3MjctZTBiMS00M2YzLWE3ZDctYTUwY2M5ZGE2YmQ3',\n",
       " 'NTIzYzRiYWEtMmI3Ny00NTMzLThkNTAtZmU0Zjg0YmEyODRm',\n",
       " 'OWM4NjM5MDctZGRjZi00NTI1LWI3NjUtNWY3MjgyMWJlMTFj',\n",
       " 'ZTRjNGM2MjgtYmFlNS00MWY3LTk3NWMtNGM0MzVlZjdhMDUz',\n",
       " 'YzM5OGRkNDEtYjJjOC00NGM0LTk2OTMtNzYxNTFmNDdiYWI2',\n",
       " 'MzBiMjI0YWUtNjc0My00YTZjLTkwYWUtOWEwZWQ1YTI4YmRi',\n",
       " 'NDJjOTkxZjgtYzQ2My00NmFmLWExM2QtY2QwYjAzYTViZGNj',\n",
       " 'YjY4OWQxM2MtN2NhOC00NWJkLTkzZTQtZjhmNDliNWNmODk5',\n",
       " 'NWNmODhjMjYtNTkwMi00NGU2LWFmMjEtNjFjMzdiNjU0ZGQz',\n",
       " 'YWRlZGZmMjAtMGYwMy00NzhjLThiZWEtNDA5MmVlZWRmZWI4',\n",
       " 'YWIyMzFlM2ItYmMxMC00MWJhLWJkYTgtMmI0MTBiY2YwYTA2',\n",
       " 'NmQ3N2FjZTItOWY1MC00MjcwLWJmYzEtY2YzYjkwNDdlM2Fl',\n",
       " 'MDQ1MGM3ODEtMzU2NC00MmI3LTgyNTctMGY5OTU2MDQ5MWYz',\n",
       " 'MzE2MmYzZWUtZGI3ZS00ODcyLTg1YWYtZjMyYjY0M2YyNjE3',\n",
       " 'NzcyNzgzM2EtNmNmZi00MGQ1LTkxOGEtNTNlMDEzNjdmOGFk',\n",
       " 'YjRmNGFjZDgtYmZhMy00MTYwLWFmMTYtMmEyZTM2OWMyMWNm',\n",
       " 'NzBkZmQ4YzMtMjk5Ni00ZjhjLTkyNzctYjJlMDUwODdiNDZh',\n",
       " 'Mzk3NWMzMzAtYzNmNy00YzVkLWJhYzctMWU1NTE3ZjBkNjRl',\n",
       " 'M2IwZmMyMjMtODJlNy00M2RhLWEwMDgtNzVmZTBlMjkxNDkx',\n",
       " 'OThhY2MxZDAtN2E5OC00NzM1LTg0YzktODRiNTkyZTdiZGRh',\n",
       " 'MTBhNGQxYTAtOGQ0MC00NmQyLWFkMzAtNjIzZmRmZTExYmRk',\n",
       " 'M2FhZDdmMWMtODkyYS00OTdlLWJkOTEtYTE3YjVkZDlmMzIw',\n",
       " 'ODdkNjA4MzktMzZlNS00Y2M0LTgyZWYtZTJkZjAzMWQwNTQ4',\n",
       " 'ZmFkYzFjZTktZWViZC00MzdkLTg2M2ItY2QzZjhjMTczZGM2',\n",
       " 'YWQyZjRiN2EtYjk3ZS00MDUzLTk2ODUtODFmMjRlYzYzMTY1',\n",
       " 'MzU4YTRjMjMtYzM3Ni00MmFkLWE4YTYtMjFhMTZhOGY5YTA3',\n",
       " 'ZTU1ZDY5NGMtM2JiOC00OTc0LThlZDEtNDliNjRjMjBmNzZi',\n",
       " 'NGIwNzAzMjEtNTk4Ni00Y2RkLWI3MWYtYjQyOWQwNThlZWVj',\n",
       " 'NTlhNzg2NzktOGZjNS00MGM0LTgzOGItM2Q3YmE2YmE5OTJi',\n",
       " 'NmJlYjZiYWQtYjBjMy00Yjk1LTg4ZTctYjljYzlmOGEwZjQ3',\n",
       " 'MmNmMThiMjAtZTRkNy00ZTdlLTk5MmYtNTYyMDA5YTMzNGEw',\n",
       " 'NzE2NTRkMTItYTIxNC00MzNlLTk4ZWQtY2E4Y2VlNjZmYTY0',\n",
       " 'ZDM0NGZmMjctZGM3OC00NDlkLWE1YzItNTJkZjc5OGE5ZjZh',\n",
       " 'MWVjMWIwNmQtMmRlNi00MzJiLWFmZjMtYmE5MGIxNGU4OTJh',\n",
       " 'MTI0MTIwN2UtNTU0Yi00NDI3LThlOGUtMzM5MzdiOGM1M2Jh',\n",
       " 'ZTEzODc3NTgtZDAzYy00NTg3LWE3MmEtYzM3ZDQ1ZDFiNTYy',\n",
       " 'NWNmNDQyZDMtYjQxNy00NmUzLWJhNzgtMjBjZjEzZjkxY2Q3']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" +\n",
    "                d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan'), 0.88353026), (Document(page_content='We call the proposed modules and optimization methods\\ntrainable bag-of-freebies.\\n1arXiv:2207.02696v1  [cs.CV]  6 Jul 2022'), 0.8826691), (Document(page_content='problem, we propose the trainable bag-of-freebies method\\nto enhance the accuracy of object detection. Based on the\\nabove, we have developed the YOLOv7 series of object de-\\ntection systems, which receives the state-of-the-art results.\\n7. Acknowledgements'), 0.87484545)]\n"
     ]
    }
   ],
   "source": [
    "# A test Query on the vector store\n",
    "res = vector_store.similarity_search_with_relevance_scores(\n",
    " query=\"What is trainable bag of features?\",\n",
    " k = 3,\n",
    " score_threshold=0.7\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiQuery Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    " retriever=vector_store.as_retriever(),llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents( query=\"What is trainable bag of features?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "We call the proposed modules and optimization methods\n",
      "trainable bag-of-freebies.\n",
      "1arXiv:2207.02696v1  [cs.CV]  6 Jul 2022\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\n",
      "detectors\n",
      "Chien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\n",
      "1Institute of Information Science, Academia Sinica, Taiwan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "can maintain the properties that the model had at the initial\n",
      "design and maintains the optimal structure.\n",
      "4. Trainable bag-of-freebies\n",
      "4.1. Planned re-parameterized convolution\n",
      "Although RepConv [13] has achieved excellent perfor-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "problem, we propose the trainable bag-of-freebies method\n",
      "to enhance the accuracy of object detection. Based on the\n",
      "above, we have developed the YOLOv7 series of object de-\n",
      "tection systems, which receives the state-of-the-art results.\n",
      "7. Acknowledgements\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "self-supervised learning or knowledge distillation methods\n",
      "that require additional data or large model. Instead, we will\n",
      "design new trainable bag-of-freebies method for the issues\n",
      "derived from the state-of-the-art methods associated with\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "Other trainable bag-of-freebies\n",
      "In this section we will list some trainable bag-of-\n",
      "freebies. These freebies are some of the tricks we used\n",
      "in training, but the original concepts were not proposed\n",
      "by us. The training details of these freebies will be elab-\n",
      "orated in the Appendix, including (1) Batch normalization\n",
      "in conv-bn-activation topology: This part mainly connects\n",
      "batch normalization layer directly to convolutional layer. The purpose of this is to integrate the mean and variance\n",
      "of batch normalization into the bias and weight of convolu-\n",
      "tional layer at the inference stage. (2) Implicit knowledge\n",
      "in YOLOR [81] combined with convolution feature map in\n",
      "addition and multiplication manner: Implicit knowledge in\n",
      "YOLOR can be simpliﬁed to a vector by pre-computing at\n",
      "the inference stage. This vector can be combined with the\n",
      "bias and weight of the previous or subsequent convolutional\n",
      "layer. (3) EMA model: EMA is a technique used in mean\n",
      "teacher [75], and in our system we use EMA model purely\n",
      "as the ﬁnal inference model.5. Experiments\n",
      "5.1. Experimental setup\n",
      "We use Microsoft COCO dataset to conduct experiments\n",
      "and validate our object detection method. All our experi-\n",
      "ments did not use pre-trained models. That is, all models\n",
      "were trained from scratch. During the development pro-\n",
      "cess, we used train 2017 set for training, and then used val\n",
      "2017 set for veriﬁcation and choosing hyperparameters. Fi-\n",
      "nally, we show the performance of object detection on the\n",
      "test 2017 set and compare it with the state-of-the-art object\n",
      "detection algorithms. Detailed training parameter settings\n",
      "are described in Appendix. We designed basic model for edge GPU, normal GPU,\n",
      "and cloud GPU, and they are respectively called YOLOv7-\n",
      "tiny, YOLOv7, and YOLOv7-W6. At the same time, we\n",
      "also use basic model for model scaling for different ser-\n",
      "vice requirements and get different types of models. For\n",
      "YOLOv7, we do stack scaling on neck, and use the pro-\n",
      "posed compound scaling method to perform scaling-up of\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual Compression Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,base_retriever=vector_store.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents( query=\"What is trainable bag of features?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Trainable bag-of-freebies\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "trainable bag-of-freebies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "trainable bag-of-freebies method\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "trainable bag-of-freebies method\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers=[retriever_from_llm,compression_retriever],weights=[0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "docs = ensemble_retriever.invoke(\"What is trainable bag of features?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Trainable bag-of-freebies\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "We call the proposed modules and optimization methods\n",
      "trainable bag-of-freebies.\n",
      "1arXiv:2207.02696v1  [cs.CV]  6 Jul 2022\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\n",
      "detectors\n",
      "Chien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\n",
      "1Institute of Information Science, Academia Sinica, Taiwan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "trainable bag-of-freebies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "self-supervised learning or knowledge distillation methods\n",
      "that require additional data or large model. Instead, we will\n",
      "design new trainable bag-of-freebies method for the issues\n",
      "derived from the state-of-the-art methods associated with\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "trainable bag-of-freebies method\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "problem, we propose the trainable bag-of-freebies method\n",
      "to enhance the accuracy of object detection. Based on the\n",
      "above, we have developed the YOLOv7 series of object de-\n",
      "tection systems, which receives the state-of-the-art results.\n",
      "7. Acknowledgements\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "can maintain the properties that the model had at the initial\n",
      "design and maintains the optimal structure.\n",
      "4. Trainable bag-of-freebies\n",
      "4.1. Planned re-parameterized convolution\n",
      "Although RepConv [13] has achieved excellent perfor-\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Tree-of-Thoughts Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template= \"\"\"\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is {question}. Answer it using only the context {context}\n",
    "A:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    " input_variables=[\"question\",\"context\"],\n",
    " template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\"\n",
    "Combine three different answers to provide the best solution out of all of them {answers}\n",
    "A:\n",
    "\"\"\"\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"answers\"],\n",
    "    template=template2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ChatOpenAI(model=\"gpt-4\")\n",
    "model_parser = model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_content(docs):\n",
    " return list(map(lambda x: x.page_content,docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    " {\"context\":ensemble_retriever |RunnableLambda(retriever_content),\"question\":RunnablePassthrough()} |prompt| {\"answers\":model_parser} | prompt2 | {\"answer\":model_parser}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\OneDrive\\Desktop\\Coding Stuff\\BTP\\env\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"Describe the model arch of Yolov7?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
